{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0072eff8-4236-491c-a6bb-01ea5a8d501a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b47d6-9aaf-46a3-b9dc-01ceb09020dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b2e006-d84a-4fc6-9c8e-fc35f1ab1aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_path=\"/pscratch/sd/l/lemonboy/PDB70_training_ver_A/eigenvalue_training/saxs_r/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3f5271-fa34-4699-b023-ab60ea8b58fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=os.path.join(training_path,'6LN0_A.pdb.pr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53efd692-9680-4a60-8644-29cca46b07ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>P(r)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.420565e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.758611e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>120.0</td>\n",
       "      <td>1.840159e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>120.5</td>\n",
       "      <td>7.102368e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>121.0</td>\n",
       "      <td>3.551184e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>121.5</td>\n",
       "      <td>6.456698e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>122.0</td>\n",
       "      <td>3.228349e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         r          P(r)\n",
       "0      0.0  0.000000e+00\n",
       "1      0.5  0.000000e+00\n",
       "2      1.0  0.000000e+00\n",
       "3      1.5  3.420565e-04\n",
       "4      2.0  1.758611e-04\n",
       "..     ...           ...\n",
       "240  120.0  1.840159e-07\n",
       "241  120.5  7.102368e-08\n",
       "242  121.0  3.551184e-08\n",
       "243  121.5  6.456698e-09\n",
       "244  122.0  3.228349e-09\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1319a2-547c-40f3-b7d9-9b67259d19f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAXSDataset(Dataset):\n",
    "    def __init__(self, csv_list):\n",
    "        self.csv_list = csv_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv_list)\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_csv(self.csv_list[idx])\n",
    "        # The first point is always zero so I didn't include it into the dataset\n",
    "        features = torch.tensor(np.pad(data['P(r)'].values[1:], (0, 512-len(data['P(r)'].values[1:])),constant_values=(0,0)), dtype=torch.float32)\n",
    "        return features, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38cee295-ea1d-411d-9acf-1afc06bd4b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "csv_list = glob.glob(training_path+'*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4052b48-5d6b-4e67-9fef-0771a3ac915c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#max_length=[]\n",
    "#for i in csv_list:\n",
    "    #pd_data=pd.read_csv(i)\n",
    "    #max_length.append(len(pd_data))\n",
    "    \n",
    "#print(max(max_length))\n",
    "#max_length.index(3179)\n",
    "#indices_of_largest_10 = sorted(range(len(max_length)), key=lambda i: max_length[i], reverse=True)[:10]\n",
    "#for i in indices_of_largest_10:\n",
    "    #print(max_length[i])\n",
    "#csv_list[42033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a127b1c3-e25d-41d8-9b65-101dcb7feffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shuffle = True\n",
    "validation_split= 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d421b94e-4369-4185-bac9-7abd03d9963e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = SAXSDataset(csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2d98ad-e246-4c04-824c-4a2baa27b87e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "num_validation_samples = int(validation_split * num_samples)\n",
    "num_train_samples = num_samples - num_validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6375d892-858e-45d8-9a8f-3d548235bfce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [num_train_samples, num_validation_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0103f2-5639-47d0-a1c3-3cefab709058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52be911b-c4f7-465a-838e-446b41a80879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7ff2362bd940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e40e9d-13d9-4e8c-8934-5c1ba7d4f56e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for batch_id, (data, target) in enumerate(train_dataloader):\n",
    "#    print(batch_id)\n",
    "#    print(\"datasize is %d\" % len(data))\n",
    "#    print(\"y size is %d\" % len(target))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e3b27-8467-4b64-a0ec-c9c7e24bfb1e",
   "metadata": {},
   "source": [
    "## VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "129fa37b-aee2-499b-8c38-153b10f428de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass VAE(nn.Module):\\n    # For P(r) the latent_size should be between 6-12. Longer sequence should have a larger\\n    # latent. For testing purpose we will set latent_size as 10\\n    \\n    def __init__(self,input_size=245, hidden_size=20, latent_size=10):\\n        super(VAE, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.latent_size = latent_size\\n        self.encoder_conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_size,\\n                                      kernel_size=3, stride=1, padding=1)\\n        # The output should be 244,20\\n        self.encoder_avgpool = nn.AvgPool1d(kernal_size=4, stride=4)\\n        # The output shoule be 61,20\\n        self.encoder_fc_mu = nn.Linear(hidden_size, latent_size)\\n        self.encoder_fc_var = nn.Linear(hidden_size, latent_size)\\n        \\n        self.decoder_fc1 = nn.Linear(latent_size, hidden_size)\\n        self.decoder_fc2 = nn.Linear(hidden_size, input_size)\\n        \\n    def encode(self, x):\\n        x = F.relu(self.encoder_conv1(x.unsqueeze(1)))\\n        x = self.encoder_maxpool(x)\\n        mu = self.encoder_fc_mu(x)\\n        log_var self.encoder_fc_var(x)\\n    \\n    def decode(self, z):\\n        z = F.relu(self.decoder_fc1(z))\\n        return torch.sigmoid(self.decoder_fc2(z))\\n    \\n    def forward(self, x):\\n\\n        x_encoded = self.encoder(x)\\n        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\\n\\n        std = torch.exp(log_var / 2)\\n        q = torch.distributions.Normal(mu, std)\\n        z = q.rsample()\\n        \\n        x_hat = self.decoder(z)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class VAE(nn.Module):\n",
    "    # For P(r) the latent_size should be between 6-12. Longer sequence should have a larger\n",
    "    # latent. For testing purpose we will set latent_size as 10\n",
    "    \n",
    "    def __init__(self,input_size=245, hidden_size=20, latent_size=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder_conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_size,\n",
    "                                      kernel_size=3, stride=1, padding=1)\n",
    "        # The output should be 244,20\n",
    "        self.encoder_avgpool = nn.AvgPool1d(kernal_size=4, stride=4)\n",
    "        # The output shoule be 61,20\n",
    "        self.encoder_fc_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.encoder_fc_var = nn.Linear(hidden_size, latent_size)\n",
    "        \n",
    "        self.decoder_fc1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.decoder_fc2 = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.encoder_conv1(x.unsqueeze(1)))\n",
    "        x = self.encoder_maxpool(x)\n",
    "        mu = self.encoder_fc_mu(x)\n",
    "        log_var self.encoder_fc_var(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = F.relu(self.decoder_fc1(z))\n",
    "        return torch.sigmoid(self.decoder_fc2(z))\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        \n",
    "        x_hat = self.decoder(z)\n",
    "'''     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7552f65-4d42-4b14-8cdb-fa42a635e04e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89856c47-74a7-4bf3-b7ac-241a44818782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a5189a8-8b43-4a0b-8dbf-9a39a8a225e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa3dd511-4e4d-4c79-86b6-6b050398c425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51f0e04f-662f-403f-8c1f-735b94bc555c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#h_=torch.bmm(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "057e19cf-0fd8-4c53-a157-71d81e55a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAXS_to_Eigen(nn.Module):\n",
    "    def __init__(self, hidden_features, output_dim):\n",
    "        super().__init__()\n",
    "        self.channels=hidden_features\n",
    "        #self.upscale = nn.Linear(1,hidden_features)\n",
    "        self.q = nn.Linear(1, hidden_features)\n",
    "        self.k = nn.Linear(1, hidden_features)\n",
    "        self.v = nn.Linear(1, hidden_features)\n",
    "        self.out_layer = nn.Linear(hidden_features, output_dim)\n",
    "        #self.orthognal_vector = nn.utils.parametrizations.orthogonal(nn.Linear(output_dim,output_dim))\n",
    "    def forward(self, x):\n",
    "        h_ = x[:, :, np.newaxis]\n",
    "        #print(x.shape, h_.shape)\n",
    "        #h_ = self.upscale(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "        w_ = torch.bmm(q,k.permute(0,2,1))\n",
    "        w_ = w_ * (self.channels**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_,dim=2)\n",
    "        h_ = torch.bmm(w_,v)\n",
    "        h_ = self.out_layer(h_)\n",
    "        h_ = nn.ReLU()(h_)\n",
    "        #Q = self.orthognal_vector.weight\n",
    "        print(h_.shape)\n",
    "        return h_\n",
    "        #return torch.matmul(h_,Q)\n",
    "        #return torch.matmul(h_,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55e25837-6975-40f9-b922-63cffb2a6ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAXS_to_Eigen_Cov(nn.Module):\n",
    "    def __init__(self, hidden_features, output_dim):\n",
    "        super().__init__()\n",
    "        self.channels=hidden_features\n",
    "        self.q = nn.Conv1d(in_channels=1, out_channels=hidden_features, kernel_size=1)\n",
    "        self.k = nn.Conv1d(in_channels=1, out_channels=hidden_features, kernel_size=1)\n",
    "        self.v = nn.Conv1d(in_channels=1, out_channels=hidden_features, kernel_size=1)\n",
    "        self.out_layer = nn.Linear(hidden_features, output_dim)\n",
    "    def forward(self, x):\n",
    "        h_ = x[:, np.newaxis, :]\n",
    "        #print(x.shape, h_.shape)\n",
    "        #h_ = self.upscale(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "        print(k.shape)\n",
    "        w_ = torch.bmm(q.permute(0,2,1),k)\n",
    "        w_ = w_ * (self.channels**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_,dim=2)\n",
    "        print(w_.shape)\n",
    "        print(v.shape)\n",
    "        h_ = torch.bmm(w_,v.permute(0,2,1))   \n",
    "        h_ = self.out_layer(h_)\n",
    "        h_ = nn.ReLU()(h_)\n",
    "        print(h_.shape)\n",
    "        return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcec55e-6345-437e-bf51-148ce11893db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68b16a97-1906-400b-b2c1-e58ec524fc70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAXS_to_Eigen_Cov(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_features, output_dim):\n",
    "        super().__init__()\n",
    "        self.channels=hidden_features\n",
    "        self.q = nn.Conv1d(in_channels=1, out_channels=hidden_features, kernel_size=1)\n",
    "        self.k = nn.Conv1d(in_channels=1, out_channels=hidden_features, kernel_size=1)\n",
    "        self.v = nn.Conv1d(in_channels=1, out_channels=hidden_features, kernel_size=1)\n",
    "        self.out_layer = nn.Linear(hidden_features, output_dim)\n",
    "        self.out_layer_2 = nn.Linear(input_shape, output_dim)\n",
    "        \n",
    "    def gram_schmidt(self, vv):\n",
    "        \n",
    "        def projection(u, v):\n",
    "            return (v * u).sum() / (u * u).sum() * u\n",
    "        \n",
    "        batch_size = vv.size(0)\n",
    "        nk = vv.size(1)\n",
    "\n",
    "        uu = torch.zeros_like(vv, device=vv.device)\n",
    "        for i in range(batch_size):\n",
    "            ui = vv[i].clone()\n",
    "            uu[i, :, 0] = ui[:, 0].clone()\n",
    "            for k in range(1, nk):\n",
    "                vk = vv[i, k].clone()\n",
    "                uk = 0\n",
    "                for j in range(0, k):\n",
    "                    uj = uu[i, :, j].clone()\n",
    "                    uk = uk + projection(uj, vk)\n",
    "                uu[i, :, k] = vk - uk\n",
    "            for k in range(nk):\n",
    "                uk = uu[i, :, k].clone()\n",
    "                uu[i, :, k] = uk / uk.norm()\n",
    "        return uu\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x[:, np.newaxis, :]\n",
    "        #print(x.shape, h_.shape)\n",
    "        #h_ = self.upscale(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "        print(k.shape)\n",
    "        w_ = torch.bmm(q.permute(0,2,1),k)\n",
    "        w_ = w_ * (self.channels**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_,dim=2)\n",
    "        print(w_.shape)\n",
    "        print(v.shape)\n",
    "        h_ = torch.bmm(w_,v.permute(0,2,1))   \n",
    "        h_ = self.out_layer(h_)\n",
    "        h_ = self.out_layer_2(h_.permute(0,2,1))\n",
    "        h_ = self.gram_schmidt(h_)\n",
    "        return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26e5ac-121a-49f2-84d4-700f87433965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69dc2c0f-ca3a-45e8-a6d7-751528a29b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch shape: torch.Size([2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gram_schmidt_batch(vv):\n",
    "    def projection(u, v):\n",
    "        return (v * u).sum() / (u * u).sum() * u\n",
    "\n",
    "    batch_size = vv.size(0)\n",
    "    nk = vv.size(1)\n",
    "    dim = vv.size(2)\n",
    "\n",
    "    uu = torch.zeros_like(vv)\n",
    "    for i in range(batch_size):\n",
    "        # Apply Gram-Schmidt process to each sample in the batch\n",
    "        u = vv[i].clone()\n",
    "        uu[i, :, 0] = u[:, 0].clone()\n",
    "        for k in range(1, nk):\n",
    "            v = vv[i, k].clone()\n",
    "            uk = 0\n",
    "            for j in range(0, k):\n",
    "                uj = uu[i, :, j].clone()\n",
    "                uk = uk + projection(uj, v)\n",
    "            uu[i, :, k] = v - uk\n",
    "        for k in range(nk):\n",
    "            uk = uu[i, :, k].clone()\n",
    "            uu[i, :, k] = uk / uk.norm()\n",
    "    return uu\n",
    "\n",
    "# Example usage\n",
    "batch_data = torch.randn(2, 256, 256)  # Example batch data with dimensions (2, 256, 256)\n",
    "processed_batch = gram_schmidt_batch(batch_data)\n",
    "print(\"Processed batch shape:\", processed_batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3591cb-d78f-4521-a7a8-b92667496aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79d7dada-d229-429e-a48b-30f7b4e7743b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SAXS_to_Eigen_Cov(512,64,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1759fc-29f3-4239-a3c7-1b73b942610e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85b35460-3ae7-4876-8977-5e6f2e47b71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = SAXS_to_Eigen(64,128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a16fbfc-438a-4562-b00d-ae9aebb6786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.0000e+00, 0.0000e+00, 8.3879e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 4.4323e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.3920e-07, 3.3187e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 2.1273e-08, 1.3552e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.7588e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.6695e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]) tensor([[0.0000e+00, 0.0000e+00, 8.3879e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 4.4323e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.3920e-07, 3.3187e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 2.1273e-08, 1.3552e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.7588e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.6695e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "for batch_id,(data,target) in enumerate(train_dataloader):\n",
    "    print(batch_id,data,target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2774dc9-9a1b-4af8-9f14-735d26cbe84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 512])\n",
      "torch.Size([2, 512, 512])\n",
      "torch.Size([2, 64, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0601, -0.0875,  0.0272,  ...,  0.0880, -0.0880,  0.0880],\n",
       "         [-0.0719, -0.0140, -0.0960,  ..., -0.0495,  0.0495, -0.0495],\n",
       "         [-0.0061,  0.0111, -0.0923,  ..., -0.1017,  0.1017, -0.1017],\n",
       "         ...,\n",
       "         [-0.0386, -0.0120,  0.0021,  ...,  0.0055, -0.0055,  0.0055],\n",
       "         [ 0.0580, -0.0331,  0.0169,  ..., -0.0880,  0.0880, -0.0880],\n",
       "         [-0.0761, -0.0308, -0.0189,  ...,  0.0055, -0.0055,  0.0055]],\n",
       "\n",
       "        [[-0.0601, -0.0875,  0.0272,  ..., -0.0964,  0.0964, -0.0964],\n",
       "         [-0.0719, -0.0140, -0.0960,  ...,  0.0999, -0.0999,  0.0999],\n",
       "         [-0.0061,  0.0111, -0.0923,  ...,  0.0482, -0.0482,  0.0482],\n",
       "         ...,\n",
       "         [-0.0386, -0.0120,  0.0021,  ..., -0.0103,  0.0103, -0.0103],\n",
       "         [ 0.0580, -0.0331,  0.0169,  ...,  0.0551, -0.0551,  0.0551],\n",
       "         [-0.0761, -0.0308, -0.0189,  ...,  0.0620, -0.0620,  0.0620]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46234388-5579-4819-8e55-bc97b58c4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "size = len(train_dataloader.dataset)\n",
    "model.train()\n",
    "for batch_id, (data, target) in enumerate(train_dataloader):\n",
    "    pred = model(data)\n",
    "    loss = loss_fn(pred, target)#Shift one digit\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'Training Batch {batch_idx}: Data shape: {data.shape}, Target shape: {target.shape}')\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), batch * batch_size + len(data)\n",
    "        print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\" )\n",
    "    if batch_id==1:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ec47a-d8e4-4a8d-b82e-d2024d8c7af5",
   "metadata": {},
   "source": [
    "## Pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f4a733b-d2a9-40de-b8a8-5033e45e4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAXSEncoderLightning(L.LightningModule):\n",
    "    def __init__(self, eigenvalue):\n",
    "        super().__init__()\n",
    "        self.eigenvalue=eigenvalue\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        loss = loss_fn(pred, target)\n",
    "        self.log(\"train_loss\",loss)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90e86b32-97a1-46b3-a635-4cb495f7a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAXS= SAXSEncoderLightning(SAXS_to_Eigen(64,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f290b9f-69d1-4b98-a2a9-58aa2332533c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer=L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5532c696-0f74-4ccd-b733-cce1a29c07d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /pscratch/sd/l/lemonboy/alphaflow_develop/alphaflow/alphaflow/notebook/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type          | Params\n",
      "---------------------------------------------\n",
      "0 | eigenvalue | SAXS_to_Eigen | 8.7 K \n",
      "---------------------------------------------\n",
      "8.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.7 K     Total params\n",
      "0.035     Total estimated model params size (MB)\n",
      "/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba25f6454bdd4d36989a3820d3b1202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAXS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/core/module.py:1303\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py:152\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:318\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/pscratch/sd/l/lemonboy/alphaflow/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m, in \u001b[0;36mSAXSEncoderLightning.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(\u001b[43mpred\u001b[49m, target)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,loss)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=SAXS, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cbbd367-4a0c-4937-9b50-c92df6cfea15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0eacc850-8fa7-4cf9-a0b9-7d953254d9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=torch.rand(64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb6f42b7-c81e-4fce-bd7a-0f4e755c29b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_orthogonal=nn.utils.parametrizations.orthogonal(nn.Linear(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e60abe4b-8f50-4b27-a1da-9e453a93561c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q = test_orthogonal.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c46229e-cde9-4195-8d14-846d69c34f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0398, -0.1448, -0.1121,  ...,  0.0513, -0.0057, -0.0592],\n",
       "        [-0.1119, -0.1731,  0.0243,  ..., -0.0794, -0.0856, -0.1963],\n",
       "        [ 0.0984,  0.0217,  0.1215,  ...,  0.2400,  0.1108, -0.0297],\n",
       "        ...,\n",
       "        [-0.0209,  0.0513, -0.0187,  ..., -0.0663,  0.1545,  0.0543],\n",
       "        [ 0.1879, -0.0800, -0.0381,  ...,  0.0574, -0.0467, -0.1177],\n",
       "        [-0.0782, -0.2289,  0.0723,  ..., -0.1609,  0.3219, -0.0163]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb08e572-4107-421e-a0bd-9ae6929c8e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9bcaf1d-7791-4f06-b72a-ebe6a6c68b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orth=test_orthogonal(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6ed914d-74bc-46aa-81a0-e005ec63e240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.9035e-08,  1.4000e-07,  ..., -2.1942e-08,\n",
       "          1.2855e-08, -3.4921e-08],\n",
       "        [ 1.9035e-08,  1.0000e+00,  5.3175e-09,  ..., -6.2299e-08,\n",
       "         -2.6486e-08,  2.7964e-08],\n",
       "        [ 1.4000e-07,  5.3175e-09,  1.0000e+00,  ..., -1.3193e-08,\n",
       "         -3.2332e-08, -3.1538e-08],\n",
       "        ...,\n",
       "        [-2.1942e-08, -6.2299e-08, -1.3193e-08,  ...,  1.0000e+00,\n",
       "         -2.0197e-08,  1.8693e-08],\n",
       "        [ 1.2855e-08, -2.6486e-08, -3.2332e-08,  ..., -2.0197e-08,\n",
       "          1.0000e+00, -3.7568e-09],\n",
       "        [-3.4921e-08,  2.7964e-08, -3.1538e-08,  ...,  1.8693e-08,\n",
       "         -3.7568e-09,  1.0000e+00]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(Q, torch.transpose(Q,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30b9ec-d42f-41cc-b1b0-e7d8c7e29ada",
   "metadata": {},
   "source": [
    "## cross attention between sequence and p(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d74737-ad4a-4998-8f7e-7504cd3a0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alphaflow",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
